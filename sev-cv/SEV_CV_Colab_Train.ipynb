{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8292077c",
   "metadata": {},
   "source": [
    "# SEV-CV: Self-Evolutionary Generative Transformers (TensorFlow, Colab)\n",
    "\n",
    "\n",
    "This Colab trains a minimal SEV-CV model with an evolutionary controller on CIFAR-10 or COCO 2017 (subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46681638",
   "metadata": {},
   "source": [
    "### How to run (Colab and Kaggle)\n",
    "1) Setup: verify TensorFlow/TFDS versions and GPU.\n",
    "2) If on Colab: Mount Google Drive and set DATA_DIR. If on Kaggle: DATA_DIR defaults to `/kaggle/working/SEV-CV/datasets`.\n",
    "3) Prepare datasets:\n",
    "   - Colab: Use TFDS into Drive, or unzip images to DATA_DIR/custom_images.\n",
    "   - Kaggle: Use the Kaggle helper cell to gdown a Google Drive link into DATA_DIR, or attach a Kaggle Dataset under `/kaggle/input` and set EXISTING_DATA_PATH.\n",
    "4) Define models (Generator/Discriminator).\n",
    "5) Define data loaders (CIFAR‑10, COCO‑2017, or a folder of images).\n",
    "6) Define evolution + training utilities.\n",
    "7) Sanity check: quick forward pass to confirm shapes.\n",
    "8) Train: pick dataset = 'cifar10' | 'coco2017' | 'folder', adjust img, batch, steps; run training.\n",
    "\n",
    "Tips:\n",
    "- On Kaggle, prefer local caches (`/kaggle/working/tfds-cache`) for speed and stability.\n",
    "- COCO uses a subset split by default; expand if you have time/space.\n",
    "- For Google Drive on Kaggle, use a public/shared link and gdown to download into DATA_DIR, then choose dataset='folder' or point TFDS to the extracted TFDS dir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00561bfe",
   "metadata": {},
   "source": [
    "## Step 1: Setup (TensorFlow + TFDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49aee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: TensorFlow + TFDS (Colab-friendly)\n",
    "import sys, subprocess, pkgutil\n",
    "\n",
    "# Optional pin for Colab stability; comment out if you want runtime default\n",
    "# subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tensorflow>=2.12,<2.17', 'tensorflow-datasets>=4.9'])\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFDS version:', tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick GPU check\n",
    "import tensorflow as tf\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "try:\n",
    "    from tensorflow.python.client import device_lib\n",
    "    print('Devices:', [d.name for d in device_lib.list_local_devices()])\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b389f",
   "metadata": {},
   "source": [
    "## Step 1: Setup (TensorFlow + TFDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8fac8",
   "metadata": {},
   "source": [
    "## Kaggle setup notes\n",
    "- In Settings: enable GPU (T4) and turn Internet ON.\n",
    "- To read a dataset from your Google Drive on Kaggle, use a public/shared link:\n",
    "  1) Quick: Use gdown to download a shared Drive file or folder into `/kaggle/working/SEV-CV/datasets`.\n",
    "  2) Faster/local: Upload a zip to Kaggle Datasets and attach it, then unzip into `DATA_DIR`.\n",
    "- This notebook prefers a local TFDS cache under `/kaggle/working/tfds-cache` when not on Colab.\n",
    "- For image folders, we place them under `DATA_DIR/custom_images` and set `dataset = 'folder'`. For TFDS-format data, point TFDS_DIR to the extracted path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle: download from Google Drive (file or folder) into DATA_DIR\n",
    "# Set MODE to 'folder' to pull a Drive folder (requires shared link) or 'file' for a single file (e.g., zip)\n",
    "import os, sys, subprocess\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if not IN_COLAB:\n",
    "    try:\n",
    "        import gdown  # type: ignore\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'gdown>=4.7.1'])\n",
    "        import gdown  # type: ignore\n",
    "\n",
    "    # User config\n",
    "    MODE = ''  # 'folder' or 'file'\n",
    "    SHARED_URL = ''  # e.g., 'https://drive.google.com/drive/folders/<FOLDER_ID>' or 'https://drive.google.com/uc?id=<FILE_ID>'\n",
    "\n",
    "    if MODE == 'folder' and SHARED_URL:\n",
    "        target_dir = os.path.join(DATA_DIR, 'custom_images')\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        print('Downloading Drive folder to', target_dir)\n",
    "        gdown.download_folder(SHARED_URL, output=target_dir, quiet=False, use_cookies=False)\n",
    "        print('Done. Set dataset = \"folder\" to train from', target_dir)\n",
    "    elif MODE == 'file' and SHARED_URL:\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        out_path = os.path.join(DATA_DIR, 'drive_file')\n",
    "        print('Downloading Drive file to', out_path)\n",
    "        gdown.download(SHARED_URL, out_path, quiet=False)\n",
    "        # Try to unzip if it's an archive\n",
    "        try:\n",
    "            import zipfile\n",
    "            if zipfile.is_zipfile(out_path):\n",
    "                with zipfile.ZipFile(out_path, 'r') as zf:\n",
    "                    zf.extractall(DATA_DIR)\n",
    "                print('Extracted under', DATA_DIR)\n",
    "            else:\n",
    "                print('Not a zip. If this is TFDS data, point EXISTING_DATA_PATH to the extracted dir and set dataset accordingly.')\n",
    "        except Exception as e:\n",
    "            print('Extraction skipped/error:', e)\n",
    "    else:\n",
    "        print('Set MODE and SHARED_URL to download from Drive, or attach a Kaggle Dataset and set EXISTING_DATA_PATH.')\n",
    "else:\n",
    "    print('On Colab, use the Drive mount cells above instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle: pull data from Google Drive link (optional)\n",
    "# Provide a shared link or file ID; this will download into DATA_DIR.\n",
    "import os, sys, subprocess\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if not IN_COLAB:\n",
    "    # Install gdown on Kaggle if needed\n",
    "    try:\n",
    "        import gdown  # type: ignore\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'gdown'])\n",
    "        import gdown  # type: ignore\n",
    "\n",
    "    # Example usage:\n",
    "    # shared_url = 'https://drive.google.com/uc?id=FILE_ID'\n",
    "    # os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    # gdown.download(shared_url, os.path.join(DATA_DIR, 'dataset.zip'), quiet=False)\n",
    "    # import zipfile\n",
    "    # with zipfile.ZipFile(os.path.join(DATA_DIR, 'dataset.zip'), 'r') as zf:\n",
    "    #     zf.extractall(DATA_DIR)\n",
    "\n",
    "    print('Kaggle helper ready. Set shared_url to your Drive file/folder export and download with gdown if needed.')\n",
    "else:\n",
    "    print('Running on Colab; use the Drive mount cells instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44d78b",
   "metadata": {},
   "source": [
    "## Step 4: Define models (SEV-G and SEV-D)\n",
    "The generator supports dynamic output sizes; ensure img_size is divisible by 4 (e.g., 32 or 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56351375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models: SEVGenerator and SEVDiscriminator (minimal)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "class MLPBlock(layers.Layer):\n",
    "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = layers.Dense(hidden)\n",
    "        self.act = layers.Activation(tf.nn.gelu)\n",
    "        self.fc2 = layers.Dense(dim)\n",
    "        self.drop = layers.Dropout(drop)\n",
    "    def call(self, x, training=False):\n",
    "        h = self.fc1(x)\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h, training=training)\n",
    "        h = self.fc2(h)\n",
    "        h = self.drop(h, training=training)\n",
    "        return h\n",
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=heads, key_dim=dim // heads)\n",
    "        self.nq = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.nk = layers.LayerNormalization(epsilon=1e-6)\n",
    "    def call(self, x, training=False):\n",
    "        q = self.nq(x)\n",
    "        k = self.nk(x)\n",
    "        return self.attn(q, k, training=training)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.n1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn = WindowAttention(dim, heads)\n",
    "        self.drop = layers.Dropout(drop)\n",
    "        self.n2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = MLPBlock(dim, mlp_ratio, drop)\n",
    "    def call(self, x, training=False):\n",
    "        h = self.attn(self.n1(x), training=training)\n",
    "        x = x + self.drop(h, training=training)\n",
    "        h = self.mlp(self.n2(x), training=training)\n",
    "        x = x + self.drop(h, training=training)\n",
    "        return x\n",
    "\n",
    "class PatchUpsample(layers.Layer):\n",
    "    def __init__(self, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = layers.Conv2DTranspose(out_ch, 4, 2, padding=\"same\")\n",
    "        self.act = layers.Activation(tf.nn.gelu)\n",
    "    def call(self, x, training=False):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "class SEVGenerator(keras.Model):\n",
    "    def __init__(self, latent_dim=128, base_dim=256, img_size=32, channels=3, depth=4, heads=4):\n",
    "        super().__init__()\n",
    "        if img_size % 4 != 0:\n",
    "            raise ValueError(\"img_size must be divisible by 4\")\n",
    "        start = img_size // 4\n",
    "        self.img_size = img_size\n",
    "        self.fc = layers.Dense(start * start * base_dim)\n",
    "        self.reshape = layers.Reshape((start, start, base_dim))\n",
    "        self.up1 = PatchUpsample(base_dim // 2)\n",
    "        self.up2 = PatchUpsample(base_dim // 4)\n",
    "        self.to_tokens = layers.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, tf.shape(x)[-1]]))\n",
    "        self.blocks = [TransformerBlock(base_dim // 4, heads) for _ in range(depth)]\n",
    "        self.to_feat = layers.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], img_size, img_size, tf.shape(x)[-1]]))\n",
    "        self.out = layers.Conv2D(channels, 1, padding=\"same\", activation=\"tanh\")\n",
    "    def call(self, z, training=False):\n",
    "        h = self.fc(z)\n",
    "        h = self.reshape(h)\n",
    "        h = self.up1(h, training=training)\n",
    "        h = self.up2(h, training=training)\n",
    "        t = self.to_tokens(h)\n",
    "        for b in self.blocks:\n",
    "            t = b(t, training=training)\n",
    "        f = self.to_feat(t)\n",
    "        return self.out(f)\n",
    "\n",
    "class SEVDiscriminator(keras.Model):\n",
    "    def __init__(self, img_size=32, channels=3, base=64, heads=4, depth=2):\n",
    "        super().__init__()\n",
    "        self.stem = keras.Sequential([\n",
    "            layers.Conv2D(base, 4, 2, padding=\"same\"), layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(base*2, 4, 2, padding=\"same\"), layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        self.flatten_hw = layers.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, tf.shape(x)[-1]]))\n",
    "        dim = base*2\n",
    "        self.blocks = [TransformerBlock(dim, heads) for _ in range(depth)]\n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "        self.out = layers.Dense(1)\n",
    "    def call(self, x, training=False):\n",
    "        h = self.stem(x, training=training)\n",
    "        t = self.flatten_hw(h)\n",
    "        for b in self.blocks:\n",
    "            t = b(t, training=training)\n",
    "        h = self.pool(t)\n",
    "        return self.out(h)\n",
    "\n",
    "def build_g(latent_dim=128, img=32, ch=3):\n",
    "    return SEVGenerator(latent_dim=latent_dim, img_size=img, channels=ch)\n",
    "\n",
    "def build_d(img=32, ch=3):\n",
    "    return SEVDiscriminator(img_size=img, channels=ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40fad5",
   "metadata": {},
   "source": [
    "## Step 5: Define data loaders (TFDS or image folder)\n",
    "These functions read from Drive-backed TFDS or a folder of images under DATA_DIR/custom_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: choose dataset source on Kaggle\n",
    "# Set one of these and run the cell:\n",
    "# - Set SHARED_URL to a Google Drive file (zip/tfrecords) to fetch via gdown\n",
    "# - Or set EXISTING_DATA_PATH to a path already present (e.g., from a Kaggle Dataset)\n",
    "import os\n",
    "SHARED_URL = ''  # e.g., 'https://drive.google.com/uc?id=FILE_ID'\n",
    "EXISTING_DATA_PATH = ''  # e.g., '/kaggle/input/your-dataset'\n",
    "\n",
    "if SHARED_URL:\n",
    "    import gdown\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    out_zip = os.path.join(DATA_DIR, 'dataset.zip')\n",
    "    print('Downloading from Drive via gdown...')\n",
    "    gdown.download(SHARED_URL, out_zip, quiet=False)\n",
    "    try:\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(out_zip, 'r') as zf:\n",
    "            zf.extractall(DATA_DIR)\n",
    "        print('Extracted to', DATA_DIR)\n",
    "    except zipfile.BadZipFile:\n",
    "        print('Downloaded file is not a zip; leaving as-is at', out_zip)\n",
    "elif EXISTING_DATA_PATH and os.path.exists(EXISTING_DATA_PATH):\n",
    "    # If TFDS-like structure exists, we can point TFDS there\n",
    "    print('Using EXISTING_DATA_PATH =', EXISTING_DATA_PATH)\n",
    "else:\n",
    "    print('Using local DATA_DIR =', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00322a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: CIFAR-10 and COCO2017 (subset)\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def _preprocess(ex, img_size):\n",
    "    x = tf.image.convert_image_dtype(ex['image'], tf.float32)\n",
    "    h = tf.shape(x)[0]\n",
    "    w = tf.shape(x)[1]\n",
    "    side = tf.minimum(h, w)\n",
    "    x = tf.image.resize_with_crop_or_pad(x, side, side)\n",
    "    x = tf.image.resize(x, [img_size, img_size], method='bilinear')\n",
    "    x = x * 2.0 - 1.0\n",
    "    return x\n",
    "\n",
    "def make_cifar10(batch=64, img=32, split='train', data_dir=None,\n",
    "                 shuffle_buf=5000, parallel_calls=2, prefetch=2):\n",
    "    ds = tfds.load('cifar10', split=split, as_supervised=False, shuffle_files=True, data_dir=data_dir)\n",
    "    ds = ds.map(lambda e: _preprocess(e, img), num_parallel_calls=parallel_calls)\n",
    "    ds = ds.shuffle(shuffle_buf).batch(batch, drop_remainder=True)\n",
    "    return ds.prefetch(prefetch)\n",
    "\n",
    "def make_coco2017(batch=32, img=128, split='train[0%:20%]', data_dir=None,\n",
    "                  shuffle_buf=2000, parallel_calls=2, prefetch=2):\n",
    "    ds = tfds.load('coco/2017', split=split, as_supervised=False, shuffle_files=True, data_dir=data_dir)\n",
    "    ds = ds.map(lambda e: _preprocess(e, img), num_parallel_calls=parallel_calls)\n",
    "    ds = ds.shuffle(shuffle_buf).batch(batch, drop_remainder=True)\n",
    "    return ds.prefetch(prefetch)\n",
    "\n",
    "# Optional: Load from a folder of images in Drive\n",
    "def _load_image_file(path, img_size):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    h = tf.shape(img)[0]\n",
    "    w = tf.shape(img)[1]\n",
    "    side = tf.minimum(h, w)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, side, side)\n",
    "    img = tf.image.resize(img, [img_size, img_size], method='bilinear')\n",
    "    img = img * 2.0 - 1.0\n",
    "    return img\n",
    "\n",
    "def make_folder_dataset(folder, batch=32, img=128, shuffle_buf=2000, parallel_calls=2, prefetch=2):\n",
    "    files = tf.data.Dataset.list_files(os.path.join(folder, '**', '*.*'), shuffle=True)\n",
    "    ds = files.map(lambda p: _load_image_file(p, img), num_parallel_calls=parallel_calls)\n",
    "    ds = ds.shuffle(shuffle_buf).batch(batch, drop_remainder=True)\n",
    "    return ds.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution controller + training utils\n",
    "import dataclasses, random\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Individual:\n",
    "    lr: float\n",
    "    heads: int\n",
    "    depth: int\n",
    "    score: float = 0.0\n",
    "\n",
    "class EvolutionController:\n",
    "    def __init__(self, population=4, seed=0):\n",
    "        random.seed(seed)\n",
    "        self.population: List[Individual] = []\n",
    "        for _ in range(population):\n",
    "            self.population.append(Individual(lr=1e-4*10**random.uniform(-0.5,0.5), heads=random.choice([2,4,6]), depth=random.choice([2,4,6])))\n",
    "    def mutate(self, ind: Individual) -> Individual:\n",
    "        return Individual(\n",
    "            lr=max(1e-5, min(5e-4, ind.lr * (1.0 + random.uniform(-0.3,0.3)))),\n",
    "            heads=max(2, min(8, ind.heads + random.choice([-2,0,2]))),\n",
    "            depth=max(2, min(8, ind.depth + random.choice([-2,0,2]))),\n",
    "        )\n",
    "    def select(self, k=2):\n",
    "        self.population.sort(key=lambda i: i.score, reverse=True)\n",
    "        self.population = self.population[:k] + [self.mutate(self.population[i%k]) for i in range(k, len(self.population))]\n",
    "\n",
    "# Losses\n",
    "@tf.function\n",
    "def d_loss_fn(real_logits, fake_logits):\n",
    "    return tf.reduce_mean(tf.nn.relu(1.0 - real_logits)) + tf.reduce_mean(tf.nn.relu(1.0 + fake_logits))\n",
    "\n",
    "@tf.function\n",
    "def g_loss_fn(fake_logits):\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "\n",
    "class HingeGAN:\n",
    "    def __init__(self, img=32, z=128, heads=4, depth=4, lr=2e-4):\n",
    "        self.G = SEVGenerator(latent_dim=z, img_size=img, channels=3, depth=depth, heads=heads)\n",
    "        self.D = SEVDiscriminator(img_size=img, channels=3, heads=heads, depth=2)\n",
    "        self.opt_g = keras.optimizers.Adam(lr, beta_1=0.0, beta_2=0.99)\n",
    "        self.opt_d = keras.optimizers.Adam(lr, beta_1=0.0, beta_2=0.99)\n",
    "        self.z = z\n",
    "    @tf.function\n",
    "    def step(self, real):\n",
    "        b = tf.shape(real)[0]\n",
    "        noise = tf.random.normal([b, self.z])\n",
    "        with tf.GradientTape() as td:\n",
    "            fake = self.G(noise, training=True)\n",
    "            r = self.D(real, training=True)\n",
    "            f = self.D(fake, training=True)\n",
    "            dl = d_loss_fn(r, f)\n",
    "        dvars = self.D.trainable_variables\n",
    "        dgrads = td.gradient(dl, dvars)\n",
    "        self.opt_d.apply_gradients(zip(dgrads, dvars))\n",
    "\n",
    "        noise = tf.random.normal([b, self.z])\n",
    "        with tf.GradientTape() as tg:\n",
    "            fake = self.G(noise, training=True)\n",
    "            f = self.D(fake, training=True)\n",
    "            gl = g_loss_fn(f)\n",
    "        gvars = self.G.trainable_variables\n",
    "        ggrads = tg.gradient(gl, gvars)\n",
    "        self.opt_g.apply_gradients(zip(ggrads, gvars))\n",
    "        return dl, gl\n",
    "\n",
    "    def sample_fitness(self, n=4):\n",
    "        z = tf.random.normal([n, self.z])\n",
    "        imgs = self.G(z, training=False)\n",
    "        # simple proxy: encourage variance\n",
    "        return float(tf.math.reduce_std(imgs).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1113",
   "metadata": {},
   "source": [
    "## Step 6: Evolution + training utilities\n",
    "Implements Hinge-GAN losses and a lightweight evolutionary controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1864ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: choose dataset and run a short evolution-guided loop\n",
    "import time, os, tensorflow as tf\n",
    "\n",
    "# Options: 'cifar10', 'coco2017', or 'folder'\n",
    "dataset = 'cifar10'  # change to 'coco2017' or 'folder'\n",
    "low_mem = True  # set True to avoid RAM crashes (esp. free Colab) and for Kaggle stability\n",
    "\n",
    "# Defaults\n",
    "img = 32 if dataset=='cifar10' else 128\n",
    "batch = 64 if dataset=='cifar10' else 32\n",
    "steps = 200\n",
    "\n",
    "# Low-memory overrides\n",
    "if low_mem:\n",
    "    if dataset=='coco2017':\n",
    "        img = 64  # downscale to cut memory\n",
    "        batch = 8  # smaller batch\n",
    "    else:\n",
    "        batch = 32\n",
    "    steps = 100\n",
    "\n",
    "# Point TFDS to Google Drive, or reuse pre-set tfds_dir (Kaggle), else local fallback\n",
    "TFDS_DIR = globals().get('tfds_dir', os.path.join(DATA_DIR, 'tfds') if 'DATA_DIR' in globals() else None)\n",
    "LOCAL_TFDS_DIR = globals().get('LOCAL_TFDS_DIR', None)\n",
    "\n",
    "# Folder-source (for dataset='folder')\n",
    "# Prefer EXISTING_DATA_PATH if set (e.g., a Kaggle Dataset or extracted Drive zip), else use DATA_DIR/custom_images\n",
    "custom_folder = os.path.join(DATA_DIR, 'custom_images') if 'DATA_DIR' in globals() else './custom_images'\n",
    "folder_src = (\n",
    "    globals().get('EXISTING_DATA_PATH') if globals().get('EXISTING_DATA_PATH') and os.path.exists(globals().get('EXISTING_DATA_PATH'))\n",
    "    else custom_folder\n",
    ")\n",
    "if dataset == 'folder':\n",
    "    os.makedirs(folder_src, exist_ok=True)\n",
    "\n",
    "# Data constructors with conservative buffers\n",
    "make_kwargs = dict(shuffle_buf=1000, parallel_calls=1, prefetch=1) if low_mem else {}\n",
    "\n",
    "def make_ds_with_fallback():\n",
    "    try:\n",
    "        if dataset=='cifar10':\n",
    "            return make_cifar10(batch=batch, img=img, split='train', data_dir=TFDS_DIR, **make_kwargs)\n",
    "        elif dataset=='coco2017':\n",
    "            split = 'train[0%:5%]' if low_mem else 'train[0%:10%]'\n",
    "            return make_coco2017(batch=batch, img=img, split=split, data_dir=TFDS_DIR, **make_kwargs)\n",
    "        else:\n",
    "            return make_folder_dataset(folder_src, batch=batch, img=img, **make_kwargs)\n",
    "    except Exception as e:\n",
    "        print('Primary data source failed; attempting fallback. Error:', e)\n",
    "        if dataset=='cifar10':\n",
    "            return make_cifar10(batch=batch, img=img, split='train', data_dir=LOCAL_TFDS_DIR, **make_kwargs)\n",
    "        elif dataset=='coco2017':\n",
    "            split = 'train[0%:5%]' if low_mem else 'train[0%:10%]'\n",
    "            return make_coco2017(batch=batch, img=img, split=split, data_dir=LOCAL_TFDS_DIR, **make_kwargs)\n",
    "        else:\n",
    "            return make_folder_dataset(folder_src, batch=batch, img=img, **make_kwargs)\n",
    "\n",
    "ds = make_ds_with_fallback()\n",
    "it = iter(ds)\n",
    "evo = EvolutionController(population=2 if low_mem else 4, seed=42)\n",
    "\n",
    "# Smaller models for low_mem\n",
    "def build_g_cfg(heads, depth):\n",
    "    if low_mem:\n",
    "        return SEVGenerator(latent_dim=96, img_size=img, channels=3, depth=max(2, depth//2), heads=max(2, heads//2), base_dim=192)\n",
    "    return SEVGenerator(latent_dim=128, img_size=img, channels=3, depth=depth, heads=heads)\n",
    "\n",
    "def build_d_cfg(heads):\n",
    "    if low_mem:\n",
    "        return SEVDiscriminator(img_size=img, channels=3, heads=max(2, heads//2), depth=1, base=48)\n",
    "    return SEVDiscriminator(img_size=img, channels=3, heads=heads, depth=2)\n",
    "\n",
    "models = []\n",
    "for ind in evo.population:\n",
    "    gan = HingeGAN(img=img, z=96 if low_mem else 128, heads=ind.heads, depth=ind.depth, lr=ind.lr)\n",
    "    # Swap in lighter nets\n",
    "    gan.G = build_g_cfg(ind.heads, ind.depth)\n",
    "    gan.D = build_d_cfg(ind.heads)\n",
    "    models.append(gan)\n",
    "\n",
    "for step in range(1, steps+1):\n",
    "    try:\n",
    "        real = next(it)\n",
    "    except Exception as e:\n",
    "        print('Iterator failed, recreating dataset/iterator. Error:', e)\n",
    "        ds = make_ds_with_fallback()\n",
    "        it = iter(ds)\n",
    "        real = next(it)\n",
    "    for gan in models:\n",
    "        dl, gl = gan.step(real)\n",
    "    if step % 25 == 0:\n",
    "        for i, (gan, ind) in enumerate(zip(models, evo.population)):\n",
    "            ind.score = gan.sample_fitness(n=2 if low_mem else 4)\n",
    "        evo.select(k=1 if low_mem else 2)\n",
    "        models = []\n",
    "        for ind in evo.population:\n",
    "            new_gan = HingeGAN(img=img, z=96 if low_mem else 128, heads=ind.heads, depth=ind.depth, lr=ind.lr)\n",
    "            new_gan.G = build_g_cfg(ind.heads, ind.depth)\n",
    "            new_gan.D = build_d_cfg(ind.heads)\n",
    "            models.append(new_gan)\n",
    "        print(f\"[step {step}] evolved heads/depth/lr:\", [(ind.heads, ind.depth, round(ind.lr,6)) for ind in evo.population])\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34802a79",
   "metadata": {},
   "source": [
    "## Step 8: Train\n",
    "Pick the dataset, adjust img/batch/steps, and run the loop. Models evolve every 25 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b60476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: forward generator at chosen resolution\n",
    "img_test = 128\n",
    "G_test = SEVGenerator(latent_dim=128, img_size=img_test, channels=3, depth=2, heads=4)\n",
    "y = G_test(tf.random.normal([2,128]), training=False)\n",
    "print('gen out shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8647a",
   "metadata": {},
   "source": [
    "## Step 7: Sanity check\n",
    "Forward the generator once at your desired resolution to confirm shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1060",
   "metadata": {},
   "source": [
    "## Mount Google Drive and set dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac194b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and set dataset folder (Colab only); set default in Kaggle\n",
    "import os, sys\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    # Customize the folder name if you like\n",
    "    DATA_DIR = '/content/drive/MyDrive/SEV-CV/datasets'\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print('DATA_DIR =', DATA_DIR)\n",
    "    print('Free space (approx):')\n",
    "    !df -h /content/drive | tail -n 1\n",
    "else:\n",
    "    # Kaggle or other environment\n",
    "    DATA_DIR = '/kaggle/working/SEV-CV/datasets'\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print('Not in Colab. Using local DATA_DIR =', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ee62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force remount Drive on Colab; on Kaggle just set local cache\n",
    "import os\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive as _drive  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.flush_and_unmount()\n",
    "    except Exception:\n",
    "        pass\n",
    "    _drive.mount('/content/drive', force_remount=True)\n",
    "    # Local fallback TFDS dir (fast, avoids Drive disconnects). You can copy prepared datasets here if needed.\n",
    "    LOCAL_TFDS_DIR = '/content/tfds-cache'\n",
    "else:\n",
    "    # Kaggle runtime: prefer fast local cache under /kaggle/working\n",
    "    LOCAL_TFDS_DIR = '/kaggle/working/tfds-cache'\n",
    "\n",
    "os.makedirs(LOCAL_TFDS_DIR, exist_ok=True)\n",
    "print('LOCAL_TFDS_DIR =', LOCAL_TFDS_DIR)\n",
    "\n",
    "# Optional: quick check of a few TFRecord shards (only meaningful on Colab Drive)\n",
    "if IN_COLAB:\n",
    "    import glob\n",
    "    coco_glob = '/content/drive/MyDrive/SEV-CV/datasets/tfds/coco/2017/1.1.0/*.tfrecord*'\n",
    "    print('Found COCO shards on Drive:', len(glob.glob(coco_glob)))\n",
    "else:\n",
    "    print('Skipping Drive shard check (not Colab).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a69f3b",
   "metadata": {},
   "source": [
    "### Troubleshooting: Drive disconnects\n",
    "If you see \"Transport endpoint is not connected\", force-remount Drive and retry. Then prefer a local runtime cache (next cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ce7d0",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive and set DATA_DIR\n",
    "This stores datasets/checkpoints persistently under MyDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use TFDS to download directly into Drive (Colab) or into local cache (Kaggle)\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Preferred TFDS directory\n",
    "if IN_COLAB:\n",
    "    tfds_dir = os.path.join(DATA_DIR, 'tfds')\n",
    "else:\n",
    "    # Kaggle: prefer LOCAL_TFDS_DIR for speed\n",
    "    tfds_dir = os.environ.get('TFDS_DIR', globals().get('LOCAL_TFDS_DIR', '/kaggle/working/tfds-cache'))\n",
    "\n",
    "os.makedirs(tfds_dir, exist_ok=True)\n",
    "print('TFDS dir:', tfds_dir)\n",
    "\n",
    "# On Kaggle, avoid large downloads unless you intend to (set DO_TFDS_DOWNLOAD=True)\n",
    "DO_TFDS_DOWNLOAD = IN_COLAB\n",
    "if DO_TFDS_DOWNLOAD:\n",
    "    _ = tfds.load('cifar10', data_dir=tfds_dir, split='train', with_info=False)\n",
    "    _ = tfds.load('coco/2017', data_dir=tfds_dir, split='train[0%:10%]', with_info=False)\n",
    "    print('TFDS prepared under', tfds_dir)\n",
    "else:\n",
    "    print('Skipping TFDS download. Ensure data exists under', tfds_dir, 'or use the Kaggle helper to fetch from Drive.')\n",
    "\n",
    "# Option B: If you already have a zip/tar, unzip into DATA_DIR\n",
    "# import zipfile\n",
    "# zip_path = '/kaggle/input/your-dataset.zip'\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "#     zf.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0c386",
   "metadata": {},
   "source": [
    "## Step 3: Prepare datasets into Drive\n",
    "Recommended: TFDS caches under DATA_DIR/tfds. You can also unzip your own images to DATA_DIR/custom_images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
