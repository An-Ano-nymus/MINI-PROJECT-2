{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0690bf39",
   "metadata": {},
   "source": [
    "# SEV-CV Mathematics and Training Overview\n",
    "\n",
    "This notebook explains the model, objectives, and the evolutionary controller.\n",
    "\n",
    "## Objectives (GAN hinge)\n",
    "- Discriminator: L_D = E[max(0, 1 - D(x))] + E[max(0, 1 + D(G(z)))]\n",
    "- Generator: L_G = - E[D(G(z))]\n",
    "\n",
    "Add-ons (future): R1 penalty, path length regularizer, wavelet loss, CLIP alignment for conditional.\n",
    "\n",
    "## Transformer Block\n",
    "- LayerNorm + Multi-Head Attention + residual\n",
    "- LayerNorm + MLP (GELU) + residual\n",
    "\n",
    "Tokens are H×W patches reshaped to N×C.\n",
    "\n",
    "## Evolutionary Controller\n",
    "- Individuals contain policy (lr, ema), micro-arch (heads, depth), and z seed.\n",
    "- Mutate: jitter lr, ema; heads/depth ±1.\n",
    "- Select: rank by fitness proxy (diversity/variance placeholder).\n",
    "\n",
    "## Training Loop Sketch\n",
    "for step in range(K):\n",
    "  - sample real batch\n",
    "  - update D on real/fake (hinge)\n",
    "  - update G to fool D\n",
    "Every T steps:\n",
    "  - evolve policies and micro-arch (phase 1+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96916ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss demo\n",
    "import tensorflow as tf\n",
    "batch = 8\n",
    "logits_real = tf.random.normal([batch, 1])\n",
    "logits_fake = tf.random.normal([batch, 1])\n",
    "d_loss = tf.reduce_mean(tf.nn.relu(1. - logits_real)) + tf.reduce_mean(tf.nn.relu(1. + logits_fake))\n",
    "g_loss = -tf.reduce_mean(logits_fake)\n",
    "d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny attention example\n",
    "import tensorflow as tf\n",
    "B,N,C,H = 2, 16, 64, 4\n",
    "x = tf.random.normal([B,N,C])\n",
    "attn = tf.keras.layers.MultiHeadAttention(num_heads=H, key_dim=C//H)\n",
    "y = attn(x, x)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
